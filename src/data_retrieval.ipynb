{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOACo4Jb5pJTmlfZxaiPgda",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Bhekmuzi/water-usage-norm/blob/main/src/data_retrieval.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip freeze > requirements.txt"
      ],
      "metadata": {
        "id": "tn2hJoUBYdoc"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install influxdb"
      ],
      "metadata": {
        "id": "N-bFSmf9YmhF",
        "outputId": "22c8452f-5710-419d-c8ba-86519479a1e6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: influxdb in /usr/local/lib/python3.10/dist-packages (5.3.1)\n",
            "Requirement already satisfied: python-dateutil>=2.6.0 in /usr/local/lib/python3.10/dist-packages (from influxdb) (2.8.2)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.10/dist-packages (from influxdb) (2023.3.post1)\n",
            "Requirement already satisfied: requests>=2.17.0 in /usr/local/lib/python3.10/dist-packages (from influxdb) (2.31.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from influxdb) (1.16.0)\n",
            "Requirement already satisfied: msgpack in /usr/local/lib/python3.10/dist-packages (from influxdb) (1.0.7)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.17.0->influxdb) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.17.0->influxdb) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.17.0->influxdb) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.17.0->influxdb) (2023.11.17)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pandas pymongo"
      ],
      "metadata": {
        "id": "uzwCwKVcZPC4",
        "outputId": "d0255e8d-3b33-41dc-c1ad-513cb7186535",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (1.5.3)\n",
            "Requirement already satisfied: pymongo in /usr/local/lib/python3.10/dist-packages (4.6.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.3.post1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.23.5)\n",
            "Requirement already satisfied: dnspython<3.0.0,>=1.16.0 in /usr/local/lib/python3.10/dist-packages (from pymongo) (2.4.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "qih7MD7MTfH8"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.cluster import KMeans\n",
        "# pip install influxdb\n",
        "# !python --version\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import datetime\n",
        "import time\n",
        "from influxdb import InfluxDBClient\n",
        "from pymongo import MongoClient\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab import files\n",
        "from sklearn.cluster import KMeans, DBSCAN\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from datetime import datetime, timedelta\n",
        "# from scipy.stats import entropy\n",
        "\n",
        "# Provide the IP address, username, password, database name, RFC3339 standard time format, and create a connection client for the 'db0' database\n",
        "client = InfluxDBClient('59.120.114.133', 8086, 'telegraf', 'telegraf', 'db0', 'rfc3339', timeout=10)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to calculate start and end times for the previous day\n",
        "def calculate_previous_day_times():\n",
        "    current_datetime = datetime.now()\n",
        "    start_of_previous_day = current_datetime - timedelta(days=1)\n",
        "    start_of_previous_day = start_of_previous_day.replace(hour=0, minute=0, second=0, microsecond=0)\n",
        "    end_of_previous_day = current_datetime.replace(hour=0, minute=0, second=0, microsecond=0) - timedelta(microseconds=1)\n",
        "\n",
        "    return start_of_previous_day, end_of_previous_day"
      ],
      "metadata": {
        "id": "Ci3hctcSVjG4"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# InfluxDB query\n",
        "start_time, end_time = calculate_previous_day_times()\n",
        "\n",
        "sql_string = f'SELECT DISTINCT(\"value\") AS value FROM mbMQTT6 WHERE \"topic\" = \\'mbMQTT2/home2127/C2BDF8/TH20\\' AND time >= \\'{start_time.strftime(\"%Y-%m-%dT%H:%M:%SZ\")}\\' AND time <= \\'{end_time.strftime(\"%Y-%m-%dT%H:%M:%SZ\")}\\' GROUP BY time(10s) FILL(previous) ORDER BY time ASC TZ(\\'Asia/Taipei\\');'\n",
        "\n",
        "result = client.query(sql_string) #"
      ],
      "metadata": {
        "id": "sa2Ga-TsVkMr"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "homes_water_data = {\n",
        "    'home2127':pd.DataFrame(result['mbMQTT6']),\n",
        "    'home2128':pd.DataFrame(result['mbMQTT6'])\n",
        "}"
      ],
      "metadata": {
        "id": "yOchQnLpV0M7"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to fill missing values for a given home\n",
        "def fill_missing_values(home_data):\n",
        "    home_data['time'] = pd.to_datetime(home_data['time'])\n",
        "    home_data.set_index('time', inplace=True)\n",
        "    expected_time_intervals = pd.date_range(start=home_data.index.min(), end=home_data.index.max(), freq='10S')\n",
        "    home_data = home_data.reindex(expected_time_intervals)\n",
        "    home_data['value'] = home_data['value'].fillna(method='pad')\n",
        "    home_data.reset_index(inplace=True)\n",
        "    return home_data"
      ],
      "metadata": {
        "id": "OmNJ7-8yV0TG"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_home_data(home_water_df):\n",
        "    # Step 1: Fill missing values\n",
        "    filled_home_data = fill_missing_values(home_water_df)\n",
        "\n",
        "    # Step 2: Convert 'value' column to numeric and perform division and multiplication\n",
        "    filled_home_data['value'] = pd.to_numeric(filled_home_data['value'], errors='coerce')\n",
        "    filled_home_data['value'] = filled_home_data['value'] / 100000 * 1000\n",
        "\n",
        "    # Step 3: Calculate the difference between 'value' column\n",
        "    filled_home_data['volume'] = filled_home_data['value'].diff()\n",
        "\n",
        "    # Step 4: Replace NaN with 0 in the 'volume' column\n",
        "    filled_home_data['volume'] = filled_home_data['volume'].fillna(0)\n",
        "\n",
        "    # # Step 5: Filter values less than 0.2 in the 'volume' column\n",
        "    # filtered_home_data = filled_home_data[filled_home_data['volume'] < 0.2]\n",
        "    # Step 5: Filter values less than 0.2 in the 'volume' column and set them to 0\n",
        "    filled_home_data['volume'] = filled_home_data['volume'].apply(lambda x: 0 if x < 0.2 else x)\n",
        "\n",
        "    # Step 6: Rename 'index' column to 'time'\n",
        "    filled_home_data.reset_index(drop=True, inplace=True)\n",
        "    filled_home_data.rename(columns={'index': 'time'}, inplace=True)\n",
        "\n",
        "    # return filtered_home_data\n",
        "    return filled_home_data\n",
        "\n"
      ],
      "metadata": {
        "id": "quoPaJHXlCN2"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage for each home\n",
        "homes_data_processed = {}\n",
        "\n",
        "for home_name, home_water_df in homes_water_data.items():\n",
        "    processed_data = process_home_data(home_water_df)\n",
        "    homes_data_processed[home_name] = processed_data\n",
        "    # print(f\"\\nProcessed Data for {home_name}:\\n{processed_data}\")\n"
      ],
      "metadata": {
        "id": "Fuzgz55LlMmU"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from datetime import timedelta\n",
        "import numpy as np\n",
        "from scipy.stats import entropy\n",
        "\n",
        "def process_events(df):\n",
        "    start_time = None\n",
        "    end_time = None\n",
        "    consecutive_zeros = 0\n",
        "    total_time = timedelta()\n",
        "    total_vol = 0\n",
        "    num_records = 0\n",
        "\n",
        "     # Create a new DataFrame to store event information\n",
        "    event_df = pd.DataFrame(columns=['Start Time', 'End Time'])\n",
        "\n",
        "    for index, row in df.iterrows():\n",
        "        time = row['time']\n",
        "        vol = max(0, float(row['volume']))\n",
        "\n",
        "        if vol != 0:\n",
        "            if start_time is None:\n",
        "                start_time = time - timedelta(seconds=60)\n",
        "            consecutive_zeros = 0\n",
        "        else:\n",
        "            consecutive_zeros += 1\n",
        "            if start_time is not None and consecutive_zeros == 6:\n",
        "                end_time = time\n",
        "                # print(f\"Event start_time: {start_time}, end_time: {end_time}\")\n",
        "\n",
        "                # Filter volume values between start_time and end_time\n",
        "                event_data = df[(df['time'] >= start_time) & (df['time'] <= end_time)]\n",
        "\n",
        "                # Calculate total time, total volume, and coefficient of variation\n",
        "                time1 = start_time + timedelta(seconds=60)\n",
        "                time2 = end_time - timedelta(seconds=60)\n",
        "                event_data1 = df[(df['time'] >= time1) & (df['time'] <= time2)]\n",
        "\n",
        "                event_df = pd.concat([event_df, pd.DataFrame({\n",
        "                    'Start Time': [time1],\n",
        "                    'End Time': [time2],\n",
        "                })], ignore_index=True)\n",
        "\n",
        "                start_time = None  # Reset start_time for the next event\n",
        "\n",
        "    if event_df.empty:\n",
        "        print(\"No events detected.\")\n",
        "        return pd.DataFrame()  # Return an empty DataFrame if no events are detected\n",
        "\n",
        "    return event_df"
      ],
      "metadata": {
        "id": "_ZYS0tdACAj1"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_and_mark_usage(home_df):\n",
        "    # Process events\n",
        "    event_df = process_events(home_df)\n",
        "\n",
        "    # Calculate the duration of each event\n",
        "    event_df['Duration'] = (event_df['End Time'] - event_df['Start Time']).dt.total_seconds()\n",
        "\n",
        "    # Filter out events with zero duration\n",
        "    event_df = event_df[event_df['Duration'] != 0]\n",
        "\n",
        "    # Filter events with duration less than 10 seconds\n",
        "    event_atleast_10_df = event_df[event_df['Duration'] >= 10]\n",
        "\n",
        "    # Initialize 'usage' column in home_df\n",
        "    home_df['usage'] = 0\n",
        "\n",
        "    # Iterate through rows in event_atleast_10_df and mark corresponding rows in home_df as 1\n",
        "    for index, row in event_atleast_10_df.iterrows():\n",
        "        mask = (home_df['time'] >= row['Start Time']) & (home_df['time'] <= row['End Time'])\n",
        "        home_df.loc[mask, 'usage'] = 1\n",
        "\n",
        "    return home_df\n"
      ],
      "metadata": {
        "id": "b0I3Oq-xMl9U"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from pymongo import MongoClient\n",
        "\n",
        "def process_resample_insert(home_df, home_id):\n",
        "    # Step 1: Process and mark usage\n",
        "    processed_home_df = process_and_mark_usage(home_df)\n",
        "    processed_home_df['time'] = pd.to_datetime(processed_home_df['time'])\n",
        "\n",
        "    # Step 2: Resample to 15-minute intervals and take max values within each interval\n",
        "    resampled_df = processed_home_df.set_index('time').resample('15T').max().reset_index()\n",
        "\n",
        "    # Extract the date and add it as a new column\n",
        "    resampled_df['date'] = resampled_df['time'].dt.date\n",
        "\n",
        "    # Drop the 'value' column\n",
        "    resampled_df = resampled_df.drop(columns=['value', 'time', 'volume'])\n",
        "\n",
        "    # Calculate active_score\n",
        "    active_score = round(resampled_df[\"usage\"].sum() / 96 * 100, 3)\n",
        "\n",
        "    date\n",
        "\n",
        "    # Group by 'date' and aggregate 'usage' column as a list\n",
        "    grouped_df = resampled_df.groupby('date')['usage'].apply(list).reset_index()\n",
        "\n",
        "    # Group by 'date' and aggregate 'usage' column as a list\n",
        "    grouped_df = resampled_df.groupby('date')['usage'].apply(list).reset_index()\n",
        "\n",
        "    # Convert DataFrame to a dictionary\n",
        "    result_dict = {}\n",
        "\n",
        "    for index, row in grouped_df.iterrows():\n",
        "        # result_dict[row['date']] = {\n",
        "        result_dict = {\n",
        "            'date': row['date'],\n",
        "            'homeID': home_id,\n",
        "            'usage': row['usage'],\n",
        "            'active_score': active_score\n",
        "        }\n",
        "\n",
        "    return result_dict\n",
        "\n",
        "# Example usage of the function\n",
        "resampled_home_data = {}\n",
        "\n",
        "for home_name, filtered_renamed_home_data in homes_data_processed.items():\n",
        "    resampled_data = process_resample_insert(filtered_renamed_home_data, home_name)\n",
        "    resampled_home_data[home_name] = resampled_data\n",
        "    print(f\"\\nResampled Data for {home_name}:\\n{resampled_data}\")"
      ],
      "metadata": {
        "id": "_taRS8mMjVJH",
        "outputId": "8c710ea6-e9be-4dae-9856-11b4e011bba6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Resampled Data for home2127:\n",
            "{'date': datetime.date(2024, 1, 10), 'homeID': 'home2127', 'usage': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0], 'active_score': 27.083}\n",
            "\n",
            "Resampled Data for home2128:\n",
            "{'date': datetime.date(2024, 1, 10), 'homeID': 'home2128', 'usage': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0], 'active_score': 27.083}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "resampled_data['usage']"
      ],
      "metadata": {
        "id": "TUyLwshPvjsm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Replace 'your_database' and 'your_collection' with your actual database and collection names\n",
        "client = MongoClient('mongodb://34.81.144.96:27017/')\n",
        "db = client['Taipower']\n",
        "collection = db['SmartWaterMeterActiveHistory']\n",
        "\n",
        "# Insert the resampled_home_data into the MongoDB collection\n",
        "for home_name, resampled_data in resampled_home_data.items():\n",
        "    for date, data in resampled_data.items():\n",
        "        collection.insert_one(data)\n",
        "\n",
        "# Close the MongoDB connection\n",
        "client.close()"
      ],
      "metadata": {
        "id": "UzJ4gHikTC0K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Query data from mongodb\n",
        "\n",
        "# print(db.list_collection_names())\n",
        "# client.close()\n",
        "# Specify the homeid you want to query\n",
        "homeid = \"Home2127\"\n",
        "\n",
        "# Calculate the date of the previous day\n",
        "end_date = datetime.now()\n",
        "start_date  = (datetime.now() - timedelta(days=7))\n",
        "\n",
        "# Query for documents with the specified homeid\n",
        "query = {\n",
        "    \"home_id\": homeid,\n",
        "    \"date\": {\"$gte\": start_date.strftime('%Y-%m-%d'),\n",
        "             \"$lte\": end_date.strftime('%Y-%m-%d')}\n",
        "}\n",
        "# query = {\"home_id\": homeid, \"date\": previous_day}\n",
        "result = collection.find(query)\n",
        "\n",
        "# Print the results\n",
        "for document in result:\n",
        "    print(document)\n",
        "\n",
        "# Close the connection\n",
        "client.close()"
      ],
      "metadata": {
        "id": "JxWmdWUqkykT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert dictionary back to usable data\n"
      ],
      "metadata": {
        "id": "mvqDKu9buYAw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate active scores, Norms, and correlation coefficient\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "def calculate_metrics(input_df):\n",
        "    # Sum the columns\n",
        "    norm_sum = input_df.sum()\n",
        "\n",
        "    # Calculate active score\n",
        "    active_score = input_df.sum() / (len(input_df) * len(input_df.columns)) * 100\n",
        "\n",
        "    # Sort DataFrame in ascending order by the first column\n",
        "    sorted_df = input_df.sort_values(by=input_df.columns[0], ascending=True)\n",
        "\n",
        "    # Find the index to split the DataFrame into two halves\n",
        "    split_index = len(sorted_df) // 2\n",
        "\n",
        "    # Split the DataFrame into the first and second halves\n",
        "    first_half = sorted_df.iloc[:split_index, :]\n",
        "    second_half = sorted_df.iloc[split_index:, :]\n",
        "\n",
        "    # Calculate the averages for each half\n",
        "    low_norm = first_half.mean()\n",
        "    high_norm = second_half.mean()\n",
        "\n",
        "    # Calculate overall norm\n",
        "    overall_norm = sorted_df.mean()\n",
        "\n",
        "    return {\n",
        "        'active_score': active_score,\n",
        "        'low_norm': low_norm,\n",
        "        'norm': overall_norm,\n",
        "        'high_norm': high_norm\n",
        "    }\n",
        "\n",
        "# Example usage\n",
        "# Assuming 'two_week_norm' is your DataFrame\n",
        "result = calculate_metrics(two_week_norm)\n",
        "\n",
        "# Print the result\n",
        "print(result)\n"
      ],
      "metadata": {
        "id": "Vk5FYduquklq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}